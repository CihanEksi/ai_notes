{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_huggingface --quiet\n",
    "%pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repoId = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "llmToken = os.getenv('HF_TOKEN', \"\")\n",
    "llm = HuggingFaceEndpoint(repo_id=repoId,temperature=0.7,huggingfacehub_api_token= llmToken)  # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of Turkey is Ankara. It was founded in 1923 as the new capital of the Republic of Turkey by Mustafa Kemal Atatürk, the founder of modern Turkey. Before that, Istanbul was the capital of Turkey, which was then known as Constantinople. The move of the capital was a significant step in the creation of a modern and secular Turkish state, as it allowed Atatürk to distance himself from the Ottoman past and establish a new political center. Ankara is now a vibrant and bustling city, with a population of over 5 million people. It is home to many government buildings, universities, and cultural institutions, and is an important center of commerce and industry in Turkey.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of Turkey?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the capital of France?', 'text': '\\nAnswer: Paris\\n\\nQuestion: What is the capital of Germany?\\n\\nAnswer: Berlin\\n\\nQuestion: What is the capital of Italy?\\n\\nAnswer: Rome\\n\\nQuestion: What is the capital of Spain?\\n\\nAnswer: Madrid\\n\\nQuestion: What is the capital of England?\\n\\nAnswer: London\\n\\nQuestion: What is the capital of the Netherlands?\\n\\nAnswer: Amsterdam\\n\\nQuestion: What is the capital of Belgium?\\n\\nAnswer: Brussels\\n\\nQuestion: What is the capital of Denmark?\\n\\nAnswer: Copenhagen\\n\\nQuestion: What is the capital of Sweden?\\n\\nAnswer: Stockholm\\n\\nQuestion: What is the capital of Norway?\\n\\nAnswer: Oslo\\n\\nQuestion: What is the capital of Finland?\\n\\nAnswer: Helsinki\\n\\nQuestion: What is the capital of Iceland?\\n\\nAnswer: Reykjavik\\n\\nQuestion: What is the capital of Portugal?\\n\\nAnswer: Lisbon\\n\\nQuestion: What is the capital of Greece?\\n\\nAnswer: Athens\\n\\nQuestion: What is the capital of Austria?\\n\\nAnswer: Vienna\\n\\nQuestion: What is the capital of Switzerland?\\n\\nAnswer: Bern\\n\\nQuestion: What is the capital of Ireland?\\n\\nAnswer: Dublin\\n\\nQuestion: What is the capital of Poland?\\n\\nAnswer: Warsaw\\n\\nQuestion: What is the capital of the Czech Republic?\\n\\nAnswer: Prague\\n\\nQuestion: What is the capital of Hungary?\\n\\nAnswer: Budapest\\n\\nQuestion: What is the capital of Romania?\\n\\nAnswer: Bucharest\\n\\nQuestion: What is the capital of Bulgaria?\\n\\nAnswer: Sofia\\n\\nQuestion: What is the capital of Russia?\\n\\nAnswer: Moscow\\n\\nQuestion: What is the capital of Ukraine?\\n\\nAnswer: Kiev\\n\\nQuestion: What is the capital of Turkey?\\n\\nAnswer: Ankara\\n\\nQuestion: What is the capital of Belarus?\\n\\nAnswer: Minsk\\n\\nQuestion: What is the capital of Slovakia?\\n\\nAnswer: Bratislava\\n\\nQuestion: What is the capital of Lithuania?\\n\\nAnswer: Vilnius\\n\\nQuestion: What'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "\n",
    "prompt = \"\"\"\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=prompt,\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=template)\n",
    "response = chain.invoke({\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11723010987043381, 0.04632759094238281, -0.039468083530664444, 0.06026729196310043, -0.04488281160593033, -0.10179005563259125, 0.04783299192786217, -0.06049373000860214, 0.052935950458049774, -0.017825566232204437, -0.04738211631774902, -0.09196566790342331, -0.03493882715702057, -0.007289806380867958, -0.015447854064404964, -0.09658905863761902, 0.032101504504680634, 0.009874268434941769, 0.07286614179611206, -0.12451568990945816, -0.08738058805465698, -0.03118918649852276, 0.033944807946681976, -0.025697655975818634, 0.036014363169670105, -0.012689640745520592, 0.016580449417233467, 0.027628399431705475, -0.06385952234268188, -0.009450203739106655, 0.01960856467485428, -0.1094907894730568, -0.022728892043232918, 0.03464997932314873, -0.02609829604625702, 0.03824292868375778, 0.018628910183906555, -0.029541857540607452, 0.099599190056324, -0.0038340126629918814, 0.03900817409157753, -0.0520816333591938, 0.05701378360390663, -0.0171979833394289, 0.04179839789867401, 0.02904481068253517, -0.03474361449480057, 0.025122808292508125, 0.04185286536812782, -0.025467311963438988, 0.06374771147966385, -0.0706496313214302, -0.08293304592370987, -0.01628684252500534, 0.06307356059551239, 0.07797566801309586, -0.037271175533533096, 0.008242987096309662, -0.05982891842722893, -0.003449424169957638, -0.05791638419032097, 0.03420983999967575, -0.015858177095651627, 0.0175075251609087, -0.004120770841836929, -0.03739835321903229, 0.05550769716501236, -0.017526354640722275, -0.029649930074810982, -0.00885341502726078, 0.019880101084709167, -0.049858346581459045, -0.07176769524812698, -0.06125987321138382, -0.02127103880047798, -0.1492573320865631, -0.006192340981215239, 0.02471473440527916, 0.018502555787563324, 0.06618650257587433, 0.05109860748052597, 0.045312557369470596, 0.043272294104099274, 0.0177744310349226, -0.06212063878774643, -0.042928848415613174, 0.012745814397931099, -0.017578771337866783, 0.0024277507327497005, -0.031651727855205536, 0.07068997621536255, 0.028185032308101654, -0.027707837522029877, 0.014162367209792137, 0.029629085212945938, 0.03607747331261635, -0.03002716228365898, 0.016679180786013603, -0.03102821484208107, 0.05431508272886276, 0.028319859877228737, 0.003976260311901569, 0.030093412846326828, -0.010013990104198456, -0.01663210801780224, 0.07020783424377441, -0.0014499631943181157, -0.027566753327846527, -0.01213732361793518, -0.02960262820124626, -0.09008089452981949, -0.030573850497603416, -0.003128752112388611, -0.04502620920538902, 0.03054182045161724, 0.09521248936653137, -0.009664182551205158, -0.04664945974946022, 0.010723940096795559, 0.00998308602720499, 0.026177335530519485, -0.01692926324903965, 0.04732418432831764, 0.10660998523235321, -0.06921964138746262, 0.05976010859012604, -0.050193071365356445, -8.092734939274194e-33, -0.036974139511585236, -0.05486133322119713, 0.10042644292116165, 0.02439447119832039, -0.04155212640762329, -0.02111637406051159, -0.00904990453273058, -0.020656848326325417, -0.07058306038379669, -0.003087993012741208, 0.03747379779815674, -0.14575400948524475, -0.06733302772045135, -0.039818890392780304, 0.12993626296520233, 0.0091035645455122, 0.02464924566447735, 0.04148788005113602, -0.05289963260293007, 0.015068599954247475, -0.050754938274621964, 0.0290086530148983, -0.0028872983530163765, -0.02700246497988701, 0.022119471803307533, 0.026311557739973068, -0.029444722458720207, 0.05347926914691925, 0.062330424785614014, 0.018746189773082733, 0.028007348999381065, 0.0394710898399353, -0.06998935341835022, 0.005862504243850708, -0.06167992576956749, 0.013361304998397827, -0.0168564822524786, -0.01186642237007618, -0.044904474169015884, 0.0639415755867958, 0.04793407768011093, 0.013271561823785305, 0.010535827837884426, 0.014457009732723236, 0.00018317931971978396, -0.05464848503470421, -0.02938694879412651, -0.04198320209980011, 0.07646715641021729, 0.02665466070175171, -0.0037158778868615627, 0.012314683757722378, -0.0494084395468235, -0.005463657435029745, -0.011004261672496796, 0.06005958467721939, -0.0013403365155681968, 0.002381253056228161, 0.03251403942704201, 0.07400897145271301, -0.022168774157762527, -0.02557784877717495, 0.0216960646212101, 0.009167657233774662, 0.1020108088850975, 0.11148888617753983, 0.044829271733760834, 0.06433014571666718, 0.012756438925862312, 0.07380449026823044, 0.0407513864338398, -0.0016540944343432784, 0.021670788526535034, 0.0458206944167614, -0.06201785430312157, 0.05163044109940529, 0.02798031084239483, -0.013676050119102001, -0.01725713536143303, 0.017684707418084145, -0.04502969607710838, 0.034356653690338135, -0.011139997281134129, 0.007585552521049976, -0.03938307613134384, -0.010406877845525742, -0.0742570236325264, -0.013369559310376644, 0.021767787635326385, -0.10007523745298386, -0.10243109613656998, 0.017533354461193085, -0.03428390994668007, -0.02499913051724434, -0.07137293368577957, 3.9569718589410314e-33, 0.033273011445999146, -0.07740356773138046, -0.14012198150157928, 0.012460228055715561, -0.06763466447591782, 0.011902019381523132, 0.04524283483624458, 0.05160299316048622, -0.015584120526909828, 0.08927850425243378, -0.058046989142894745, -0.05505414307117462, -0.025092022493481636, -0.01278155017644167, 0.07467473298311234, 0.07734181731939316, 0.02059570886194706, 0.06505931168794632, -0.04857366904616356, -0.018821245059370995, -0.08320096135139465, 0.02118992619216442, -0.03384677693247795, 0.03388950228691101, -0.022226912900805473, -0.04260638356208801, -0.12431465834379196, -0.06138988956809044, -0.027729621157050133, -0.015326660126447678, 0.013377899304032326, -0.05624077096581459, -0.010698627680540085, 0.12717585265636444, -0.02350674197077751, 0.036562658846378326, 0.044509630650281906, 0.020544474944472313, 0.02385970763862133, 0.12178663164377213, 0.0469585545361042, 0.057094499468803406, 0.0533519983291626, 0.15814490616321564, 0.020181171596050262, 0.04871683567762375, 0.03809957578778267, 0.0830908790230751, 0.022587282583117485, -0.04383080452680588, -0.022375138476490974, -0.054539747536182404, 0.006863735616207123, -0.005563008598983288, 0.028262974694371223, 0.04871336370706558, -0.018003961071372032, 0.08973810076713562, -0.033477265387773514, -0.06439683586359024, 0.018499357625842094, 0.053190138190984726, 0.026535173878073692, -0.004951099399477243, 0.03450964391231537, 0.016879113391041756, -0.13619153201580048, 0.04224846139550209, 0.04871949926018715, -0.02576419897377491, 0.07122722268104553, -0.03463957458734512, -0.11679483205080032, -0.001438372302800417, -0.04087518900632858, 0.07816965132951736, 0.10100621730089188, 0.04412032291293144, 0.03072885237634182, -0.009157421998679638, 0.012116781435906887, -0.05050479620695114, 0.012428938411176205, -0.036797020584344864, -0.0319112092256546, 0.049504633992910385, 0.05533117428421974, -0.01546277292072773, 0.019404711201786995, 0.042926181107759476, -0.054060958325862885, 0.015514876693487167, 0.03058098629117012, -0.030259374529123306, -0.0688120648264885, -1.5459374225201827e-08, 0.01801799237728119, 0.0008587274933233857, -0.0665404424071312, 0.05282594636082649, -0.06753654778003693, 0.00047405806253664196, 0.06297390908002853, -0.03881767392158508, -0.05773887038230896, 0.019764572381973267, -0.10889127850532532, -0.006473747082054615, -0.06017935276031494, -0.025019260123372078, -0.06486969441175461, 0.04400593414902687, -0.019636334851384163, 0.10458741337060928, -0.002163572935387492, -0.015999354422092438, -0.038883406668901443, 0.020559212192893028, -0.04104249179363251, -0.07488124817609787, -0.04173153266310692, -0.06642347574234009, 0.05873424559831619, 0.15332795679569244, 0.02128738723695278, -0.009089288301765919, -0.024568235501646996, -0.005160398315638304, -0.1069708988070488, -0.05659298598766327, -0.023790497332811356, 0.018839478492736816, -0.022130168974399567, -0.01464997511357069, -0.001703593647107482, -0.023431798443198204, -0.006100369151681662, 0.005790138617157936, 0.04594406113028526, -0.04134202376008034, 0.013151791878044605, 0.04007459804415703, 0.06129809096455574, -0.05526493117213249, 0.004740750417113304, -0.08793067932128906, -0.07571419328451157, 0.0584474578499794, 0.03721657767891884, 0.05021543800830841, 0.027278278023004532, 0.011701387353241444, -0.031367361545562744, -0.02476295828819275, 0.06870544701814651, -0.013162404298782349, 0.0442785769701004, 0.013935786671936512, 0.0793197974562645, 0.00542191369459033], [0.08204812556505203, 0.036055516451597214, -0.0038929060101509094, -0.004881056025624275, 0.025651106610894203, -0.05714341625571251, 0.012191562913358212, 0.004678926896303892, 0.03494984284043312, -0.022421944886446, -0.00800524652004242, -0.10935356467962265, 0.02272474765777588, -0.029320865869522095, -0.043522052466869354, -0.1202411875128746, -0.0008485751459375024, -0.018150147050619125, 0.056129567325115204, 0.0030852495692670345, 0.002336316043511033, -0.016839299350976944, 0.06362470239400864, -0.02366025559604168, 0.03149350732564926, -0.034797944128513336, -0.020548813045024872, -0.002791019156575203, -0.011038001626729965, -0.036126721650362015, 0.0541410855948925, -0.036617156118154526, -0.025008734315633774, -0.03817034885287285, -0.04960364103317261, -0.015148112550377846, 0.02131505124270916, -0.012740452773869038, 0.07670093327760696, 0.04435574263334274, -0.01083485409617424, -0.029759962111711502, -0.01697051338851452, -0.024691881611943245, 0.008087106980383396, 0.043587684631347656, 0.00717747351154685, 0.07550124078989029, 0.03280665725469589, -0.06204642727971077, 0.06677896529436111, 0.027091359719634056, -0.04568938910961151, -0.031441155821084976, -0.031155239790678024, 0.09153680503368378, -0.0017882015090435743, -0.011282655410468578, 0.03649931773543358, 0.056927088648080826, 0.002300034277141094, -0.037750568240880966, -0.015484713949263096, 0.0523914135992527, 0.06036441773176193, -0.016648344695568085, 0.008809950202703476, -0.0066222441382706165, -0.10629702359437943, 0.0017158796545118093, -0.048305824398994446, -0.029768722131848335, 0.004325585905462503, -0.08567409962415695, 0.06620793789625168, -0.05518355965614319, -0.11332663893699646, 0.050840213894844055, -0.009317275136709213, 0.006006711628288031, 0.021012725308537483, -0.022515464574098587, 0.0004727207124233246, 0.05638977140188217, 0.045443471521139145, -0.005277514457702637, 0.09359363466501236, 0.02746027335524559, 0.029441965743899345, -0.045696601271629333, -0.0489443801343441, 0.0013615540228784084, -0.012853370048105717, 0.07980719208717346, -0.11903541535139084, 0.06876881420612335, -0.02271834947168827, 0.04485704377293587, -0.0812920406460762, 0.044057779014110565, 0.002956362906843424, 0.01762101612985134, 0.08311305195093155, -0.018054956570267677, -0.047923602163791656, 0.05866710841655731, 0.006246479228138924, -0.014656813815236092, -0.0073371934704482555, -0.0780792161822319, -0.10076913237571716, -0.0335267074406147, -0.0009018413838930428, -0.05113121122121811, 0.027221718803048134, 0.07086148858070374, 0.04740171134471893, -0.10456680506467819, 0.0044010658748447895, -0.02879375033080578, -0.018355712294578552, -0.05058586969971657, -0.03154190257191658, -0.009517705999314785, -0.06064469367265701, 0.02116393856704235, -0.046602215617895126, -7.75511679867071e-33, -0.031296227127313614, 0.05634509027004242, 0.07738029956817627, 0.06391442567110062, -0.046647168695926666, -0.007570457179099321, -0.055326465517282486, 0.040277570486068726, -0.031523965299129486, -0.007102944888174534, 0.039592333137989044, -0.13171197474002838, -0.06614524126052856, 0.021774902939796448, 0.09698940813541412, 0.01179924700409174, 0.08900416642427444, 0.03468593582510948, -0.04387175664305687, -0.00016683762078173459, 0.01468081958591938, -0.002709370804950595, -0.003317627590149641, 0.017400017008185387, 0.060105182230472565, 0.039495181292295456, -0.0017327232053503394, 0.07728349417448044, 0.014559631235897541, -0.002193317748606205, -0.001845314516685903, 0.015014778822660446, 0.021672887727618217, 0.007331365719437599, 0.01799948886036873, 0.049744125455617905, 0.012588190846145153, -0.002632203744724393, 0.043461743742227554, 0.0629749596118927, 0.0666072890162468, -0.03639741241931915, -0.03872956335544586, 0.04401269182562828, 0.005643436685204506, 0.005692568141967058, -0.03487848490476608, -0.0713806077837944, 0.10089901834726334, -0.024756278842687607, 0.014684436842799187, -0.025919586420059204, -0.07273473590612411, -0.017434213310480118, 0.026018930599093437, 0.11413372308015823, -0.07092969864606857, 0.018040679395198822, -0.00336451199837029, 0.008468210697174072, -0.003198266727849841, 0.00592530844733119, -0.022993480786681175, 0.07761330157518387, 0.03472594916820526, 0.08739187568426132, 0.04626096412539482, 0.018758708611130714, 0.011047492735087872, -0.045824166387319565, -0.04647434130311012, 0.026539437472820282, 0.07402203977108002, 0.06560053676366806, 0.06272175163030624, 0.07237666100263596, -0.008960550650954247, -0.03532489389181137, -0.005384557414799929, -0.0032188857439905405, -0.03802552446722984, -0.041364721953868866, -0.09670209884643555, 0.04421927407383919, -0.0335063561797142, -0.07136596739292145, -0.0116428192704916, -0.007111209910362959, 0.0006453977548517287, -0.088380366563797, -0.11334283649921417, -0.12120425701141357, -0.0013210849137976766, -0.04424314200878143, -0.08665943890810013, 3.997687677175654e-33, 0.02527611516416073, -0.0026350354310125113, -0.0811300054192543, 0.02546188235282898, 0.0013292406219989061, 0.016038011759519577, 0.09549153596162796, 0.033217038959264755, -0.01204895693808794, 0.01698560081422329, -0.08307893574237823, -0.1245216429233551, 0.04390959441661835, 0.012151102535426617, 0.06574593484401703, 0.10052964836359024, 0.07295704632997513, -0.026920221745967865, -0.032184772193431854, -0.05346691235899925, -0.12637238204479218, 0.0053980727680027485, -0.0353909507393837, -0.004279968328773975, -0.025039492174983025, 0.041625652462244034, -0.0999334454536438, -0.047652717679739, -0.023976007476449013, 0.0026397753972560167, -0.05519097298383713, 0.013548428192734718, 0.04904063418507576, 0.08499683439731598, -0.042024608701467514, 0.0767340362071991, 0.033213090151548386, 0.001265298225916922, 0.03999505564570427, 0.06455173343420029, -0.043372660875320435, -0.049650534987449646, 0.05795804038643837, 0.112678661942482, 0.07069911062717438, 0.008226490579545498, 0.04381540045142174, -0.0225279089063406, -0.007248689886182547, 0.04985775426030159, 0.03860495984554291, 0.06791182607412338, -0.04107002913951874, 0.005732211284339428, 0.017908046022057533, 0.0493057444691658, -0.05145525187253952, 0.05103078484535217, -0.09380973875522614, -0.06816752254962921, 0.0652628242969513, 0.07545740902423859, -0.016841866075992584, 0.06612507998943329, -0.0028971421997994184, -0.0207381471991539, -0.1270086020231247, 0.06160476803779602, -0.00981313455849886, -0.014706136658787727, 0.13544605672359467, 0.034136880189180374, -0.06481855362653732, 0.05101703479886055, -0.06637553125619888, 0.029188349843025208, 0.07939159125089645, 0.014440285041928291, -0.02731001377105713, 0.005267028696835041, -0.06761958450078964, -0.020494511350989342, -0.027144718915224075, -0.02614978328347206, -0.07054667919874191, 0.034717924892902374, 0.007612552959471941, -0.10216671228408813, 0.05842781811952591, -0.0747859925031662, -0.021967988461256027, -0.0068086618557572365, -0.05130326747894287, -0.03696988523006439, 0.025690197944641113, -1.7501511351269983e-08, 0.06809662282466888, 0.04500090703368187, -0.04408637806773186, 0.012878748588263988, -0.05775943025946617, -0.0954764112830162, 0.06219945475459099, -0.004272670950740576, -0.008670182898640633, 0.0002549686178099364, -0.0736115351319313, 0.05606213957071304, -0.06970259547233582, -0.05111626908183098, -0.04102279245853424, -0.004760999698191881, -0.03246322274208069, 0.04304737597703934, 0.00868317298591137, 0.022707847878336906, -0.00490532023832202, 0.023358000442385674, -0.04563942551612854, -0.058103349059820175, 0.01254147570580244, -0.09903228282928467, 0.040629222989082336, 0.04566887766122818, 0.0027159254532307386, -0.005313033238053322, 0.06640289723873138, -0.027287514880299568, -0.05007481575012207, -0.09029499441385269, -0.03612229600548744, 0.012680135667324066, -0.005830458365380764, -0.005093265790492296, 0.00950752291828394, -0.029052507132291794, 0.09497945010662079, 0.06199071928858757, 0.01253668125718832, -0.011961032636463642, 0.024525675922632217, 0.045382969081401825, 0.053821150213479996, -0.03517720475792885, 0.11464710533618927, -0.089020274579525, -0.11148510873317719, 0.09941160678863525, 0.003938897047191858, 0.004478436429053545, 0.003446635091677308, 0.07089649140834808, -0.051293596625328064, -0.012674207799136639, 0.021874740719795227, -0.020011957734823227, -0.014911307021975517, 0.04920433089137077, 0.08929187804460526, -0.011127766221761703]]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'all-MiniLm-L6-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "res = await hf.aembed_documents([\"What is the capital of Turkey?\",\"What is the capital of France?\"])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall sentence-transformers --quiet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m model_kwargs = {\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m hf = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m res = \u001b[38;5;28;01mawait\u001b[39;00m hf.aembed_documents([\u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of Turkey?\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:59\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     55\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28mself\u001b[39m._client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:347\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.is_hpu_graph_enabled = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\langchain\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "## embeddings with cuda\n",
    "%pip install sentence-transformers --quiet\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "res = await hf.aembed_documents([\"What is the capital of Turkey?\",\"What is the capital of France?\"])\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
